\documentclass{../../../miPlantilla}

\renewcommand{\miAsignatura}{Big Data Aplicado}
\renewcommand{\tituloTrabajo}{SMARTPARKING: Sistema Inteligente de Gestión de Aparcamiento}
\renewcommand{\imagenPortada}{portada.png}

\begin{document}

\maketitle

\section{Introducción}
\subsection{Contextualización}
En la era de las '\texttt{Smart Cities}', la gestión eificiente de recursos urbanos se ha convertido en una prioridad.
Las plazas de aparcamiento representan uno de los desafíos más significativos en entornos urbanos sobrepoblados, donde 
la congestión del tráfico y la disponibilidad de plazas de aparcamiento generan pérdidas económicas, contaminación y 
estrés en los conductores. La tecnologia \textbf{Big Data}, combinada con el \textbf{Internet de las Cosas (IoT)},
ofrece soluciones innovadoras para transformar la gestión tradicional de aparcamientos en sistemas inteligentes
capaces de optimizar el espacio disponible y mejorar la experiencia del usuario.

\subsection{Justificación}
La implementación de un sistema inteligente de gestión de aparcamiento, como \textbf{SmartParking}, es crucial para
resolver la necesidad de:

\begin{itemize}
  \item Reducir el tiempo de búsqueda de plazas de aparcamiento, disminuyendo así la congestión del tráfico y
        las emisiones de CO2.
  \item Proporcionar datos en tiempo real para la toma de decisiones tanto por parte de los conductores como de los
        gestores del parking.
  \item Crear una base histórica de datos que permita analizar patrones de uso y optimizar la gestión del espacio.
  \item Servir como caso de estudio para la aplicación de tecnologías Big Data e IoT en entornos urbanos.
\end{itemize}

\subsection{Objetivos}
\subsubsection{Objetivo General}
El objetivo general del proyecto es \textbf{diseñar, desarrollar e implementar un sistema inteligente de monitorización y
gestión de plazas de aparcamiento} que permita optimizar el uso del espacio disponible en tiempo real. Este sistema se
basa en la integración de diversas tecnologías de Big Data e Internet de las Cosas (IoT), con intención de procesar, 
almacenar y visualizar información en tiempo real.

A través de la simulación de sensores IoT, el sistema detecta el estado de cada plaza (libre u ocupada), así como datos
adicionales relacionados con las condiciones del entorno o el estado de los dispositivos, como la temperatura y el nivel
de batería. Toda esta información se transmite en tiempo real a través de una infraestructura de mensajería distribuida
que garantiza la entrega de los datos de forma segura y escalable.

\newpage

Una vez recibidos, los datos son procesados y transformados para su almacenamiento estructurado. Posteriormente, los
datos pueden visualizarse en un panel web que muestra en tiempo real el estado de las plazas, y analizarse históricamente
para sacar conclusiones relevantes.

En conjunto, el objetivo es construir una arquitectura integral capaz de manejar el flujo completo de información —desde
la generación del dato hasta su análisis—, aportando una solución moderna, flexible y escalable para la gestión de
aparcamientos inteligentes. Este enfoque no solo optimiza la disponibilidad de plazas, sino que también constituye una
base sólida para la toma de decisiones y la mejora de la movilidad urbana.

\subsubsection{Objetivos Específicos}
\begin{itemize}
  \item Configurar Apache Kafka para la ingesta de datos en tiempo real desde sensores IoT.
  \item Diseñar flujos de datos en Apache NiFi para el procesamiento y routing de la información.
  \item Implementar una base de datos NoSQL (MongoDB) para el almacenamiento eficiente de grandes volúmenes de datos, 
        así como de la gestión de plazas de aparcamiento en tiempo real.
  \item Desarrollar una aplicación web con Flask para la visualización del parking en tiempo real.
  \item Integrar Dremio para el análisis avanzado de datos y generación de informes.
  \item Validar el rendimiento del sistema bajo condiciones de carga simuladas.
\end{itemize}

\subsection{Alcance del proyecto}
El proyecto \textbf{SmartParking} se centrará en la implementación de un sistema piloto que abarque las siguientes
áreas:
\begin{itemize}
  \item Desarrollo del Backend de procesamiento de datos.
  \item Implementación de la base de datos.
  \item Creación de la interfaz web de visualización.
  \item Configuración de la plataforma de análisis de datos.
  \item Documentación técnica y memoria del proyecto.
\end{itemize}

Excluye aspectos como:
\begin{itemize}
  \item Desarrollo de hardware específico para sensores IoT.
  \item Implementación en producción real.
  \item Sistemas de pago.
  \item Aplicación móvil nativa.
\end{itemize}

\subsection{Planificación}
El proyecto se desarrollará en varias fases a lo largo de un periodo de 4 semanas, distribuidas de la siguiente manera:
\begin{itemize}
  \item Semana 1: Análisis y diseño del sistema.
  \item Semana 2: Configuración de Apache Kafka, Apache NiFi e implementación de la base de datos.
  \item Semana 3: Desarrollo del backend y la interfaz web e integración con dremio.
  \item Semana 4: Pruebas finales, desarrollo de la memoria y video demostrativo.
\end{itemize}

\section{Marco teórico}
\subsection{Big Data y su relevancia en la gestión urbana}
El Big Data se refiere al manejo y análisis de grandes volúmenes de datos que no pueden ser procesados mediante
herramientas tradicionales. En el contexto urbano, el Big Data permite la recopilación y análisis de datos en tiempo real
provenientes de diversas fuentes, como sensores IoT, cámaras y dispositivos móviles. Esta capacidad es fundamental para
la gestión eficiente de recursos urbanos, incluyendo la optimización del tráfico y la gestión de aparcamientos.

\subsection{Internet de las Cosas (IoT) en la monitorización de aparcamientos}
El Internet de las Cosas (IoT) se refiere a la interconexión de dispositivos físicos a través de internet, permitiendo
la recopilación e intercambio de datos. En el ámbito de la gestión de aparcamientos, los sensores IoT pueden detectar la ocupación
de plazas en tiempo real, proporcionando datos valiosos para la optimización del uso del espacio y la mejora de la experiencia del usuario.

\subsection{Tecnologías Big Data utilizadas}
\subsubsection{Apache Kafka}
Apache Kafka es una plataforma de streaming distribuida que permite la ingesta y procesamiento de grandes volúmenes de datos en tiempo real.
En el proyecto SmartParking, Kafka se utilizará para recibir datos de los sensores IoT y distribuirlos a los componentes del sistema.

\begin{itemize}
  \item Tópico: Categoría o canal donde se publican los mensajes.
  \item Productor: Componente que envía datos a un tópico.
  \item Consumidor: Componente que recibe datos de un tópico.
  \item Broker: Servidor que almacena y distribuye los mensajes.
\end{itemize}

\subsubsection{Apache NiFi}
Apache NiFi es una herramienta de integración de datos que facilita el flujo, transformación y enrutamiento de datos entre sistemas.
En SmartParking, NiFi se empleará para procesar los datos recibidos de Kafka y enviarlos a la base de datos y otros servicios.

\begin{itemize}
  \item Procesadores: Componentes que realizan operaciones específicas sobre los datos.
  \item Flujos de datos: Definiciones de cómo los datos se mueven y transforman dentro de NiFi.
\end{itemize}

\subsubsection{MongoDB}
MongoDB es una base de datos NoSQL orientada a documentos que ofrece alta escalabilidad y flexibilidad en el almacenamiento de datos.
En este proyecto, MongoDB se utilizará para almacenar la información de las plazas de aparcamiento en tiempo real y los datos históricos
de ocupación.

\begin{itemize}
  \item Documentos: Estructuras de datos similares a JSON que almacenan la información.
  \item Colecciones: Conjuntos de documentos relacionados.
  \item Consultas: Operaciones para recuperar y manipular datos almacenados.
  \item Índices: Estructuras que mejoran la velocidad de las consultas.
\end{itemize}

\subsubsection{Flask}
Flask es un microframework web para Python que permite el desarrollo rápido de aplicaciones web. En SmartParking, Flask se utilizará para crear
la interfaz web que mostrará el estado del aparcamiento en tiempo real.

\subsubsection{Dremio}
Dremio es una plataforma de análisis de datos que facilita la consulta y visualización de grandes volúmenes de datos. En el proyecto,
Dremio se empleará para analizar los datos almacenados en MongoDB y generar consultas sobre el uso del aparcamiento. 

\newpage

\section{Fase de análisis}
\subsection{Requisitos Funcionales}
\begin{itemize}
  \item RF1: El sistema debe capturar datos de sensores IoT en tiempo real.
  \item RF2: El sistema debe almacenar un histórico completo de los eventos.
  \item RF3: El sistema debe mantener el estado actual de cada plaza.
  \item RF4: El sistema debe visualizar la ocupación en tiempo real.
  \item RF5: El sistema debe permitir el análisis de datos históricos.
\end{itemize}

\subsection{Requisitos No Funcionales}
\begin{itemize}
  \item RNF1: El sistema debe ser escalable para manejar un aumento en el número de sensores.
  \item RNF2: El sistema debe garantizar la integridad y consistencia de los datos.
\end{itemize}

\subsection{Análisis de tecnologías}
La selección de tecnologías se basa en:

\begin{itemize}
  \item Apache Kafka: Ideal para la ingesta de datos en tiempo real debido a su alta capacidad de manejo de mensajes.
  \item Apache NiFi: Facilita la integración y procesamiento de datos con una interfaz visual intuitiva.
  \item MongoDB: Proporciona flexibilidad y escalabilidad para almacenar grandes volúmenes de datos no estructurados.
  \item Flask: Permite un desarrollo rápido y sencillo de la interfaz web.
  \item Dremio: Ofrece capacidades avanzadas de análisis y visualización de datos.
\end{itemize}

\newpage

\section{Fase de diseño}
\subsection{Arquitectura del sistema}
\subsection{Arquitectura del sistema}
La arquitectura de \textbf{SmartParking} sigue un flujo de datos continuo desde los sensores hasta la visualización. Los \textbf{sensores IoT}
detectan el estado de las plazas y envían la información a través de \textbf{Apache Kafka}, que gestiona la transmisión en tiempo real.
\textbf{Apache NiFi} procesa y enruta los datos hacia \textbf{MongoDB}, donde se almacenan tanto los eventos históricos como el estado actual.
Finalmente, la información se muestra en una interfaz web desarrollada con \textbf{Flask} y se analiza mediante \textbf{Dremio}.

Sensores IoT $\rightarrow$ Apache Kafka $\rightarrow$ Apache NiFi $\rightarrow$ MongoDB $\rightarrow$ Flask (Interfaz Web) \& Dremio (Análisis de Datos)

\subsection{Diseño de la Base de Datos}
La base de datos MongoDB se diseñará con las siguientes colecciones principales:
\begin{itemize}
  \item \textbf{Events}: Documentos que registran cada evento de ocupación o liberación de una plaza, con marca temporal y detalles del sensor.
  \item \textbf{Bays}: Documentos que representan cada plaza de aparcamiento, con su estado actual (ocupada/libre).
\end{itemize}

\subsubsection{Diseño de los documentos}
Ambas colecciones tendrán la misma estructura de documento base:
\begin{lstlisting}
{
  "_id": ObjectId("..."),
  "bay_id": "L1-A-023",
  "parking_id": "PK-CADIZ-01",
  "level": "L1",
  "occupied": true,
  "last_event_ts": "2025-10-07T10:15:30Z",
  "metrics": {
    "temperature_c": 23.4,
    "battery_pct": 78
  },
  "updated_at": "2025-10-07T10:15:31Z"
}
\end{lstlisting}

\newpage

Dónde:
\begin{itemize}
  \item \texttt{\_id}: Identificador único del documento generado por MongoDB.
  \item \texttt{bay\_id}: Identificador único de la plaza de aparcamiento.
  \item \texttt{parking\_id}: Identificador del parking al que pertenece la plaza.
  \item \texttt{level}: Nivel del parking donde se encuentra la plaza.
  \item \texttt{occupied}: Estado actual de la plaza (ocupada o libre).
  \item \texttt{last\_event\_ts}: Marca temporal del último evento registrado para la plaza.
  \item \texttt{metrics}: Objeto que contiene métricas adicionales proporcionadas por el sensor.
  \begin{itemize}
    \item \texttt{temperature\_c}: Temperatura medida por el sensor en grados Celsius.
    \item \texttt{battery\_pct}: Porcentaje de batería restante del sensor
  \end{itemize} 
  \item \texttt{updated\_at}: Marca temporal de la última actualización del documento.
\end{itemize}

\subsection{Diseño del Flujo NiFi}
El flujo de datos en Apache NiFi se diseñará para:
\begin{itemize}
  \item Consumir mensajes desde el tópico de Kafka.
  \item Procesar y transformar los datos según sea necesario.
  \item Enviar los datos procesados a MongoDB para su almacenamiento en ambas colecciones.
\end{itemize}

\subsection{Diseño de la Interfaz Web}
La interfaz web desarrollada con Flask mostrará:
\begin{itemize}
  \item Mapa en tiempo real del estado de las plazas de aparcamiento.
  \item Codificación de colores: verde (libre), rojo (ocupado).
  \item Estadísticas básicas sobre la ocupación del parking.
  \item Diseño responsive para múltiples dispositivos
\end{itemize}

\newpage

\section{Fase de implementación}

\subsection{Configuración del entorno}
Para la implementación del sistema SmartParking, he creado una máquina virtual con las siguientes características:
\begin{itemize}
  \item Sistema Operativo: Lubuntu 24.04.03 LTS
  \item Memoria RAM: 8 GB
  \item Almacenamiento: 50 GB SSD
  \item Procesador: 4 núcleos
\end{itemize}

Tras la creación de la máquina, instalé algunos paquetes necesarios para su posterior uso, como Python y sus componentes.\\
{\small(Anexo: \nameref{anexo:paquetes-vm})}.

Posteriormente, instalé Java en su versión 11 para el sistema, y Java 21 para su uso en NiFi y Kafka.
\fig[1\textwidth]{Preparación de la VM/Screenshot_8.png}
\begin{center}
  {\small(Anexo: \nameref{anexo:config-java})}.
\end{center}

Finalmente, para finalizar con la configuración del sistema, creé el directorio dónde voy a trabajar y montar casi todo el sistema.
\fig[1\textwidth]{Preparación de la VM/Screenshot_9.png}

\subsection{Apache Kafka}
Con el sistema listo, lo primero que hice fue preparar Apache Kafka para su posterior uso, dado que es el primer componente de este proyecto.

Tras descargarlo y ubicarlo en el directorio de trabajo, añadí la variable de entorno \texttt{KAFKA\_HOME} al archivo \texttt{.bashrc} {\small(Anexo: \nameref{anexo:kafka})}.

\newpage

Posteriormente, configuré Kafka, para usarlo en modo \textbf{KRaft} (sin Zookeeper):
\fig[1\textwidth]{Kafka/Screenshot_4.png}
\begin{center}
  {\small(Anexo: \nameref{anexo:kafka})}.
\end{center}

Tras realizar la configuración de Apache Kafka tal y como se puede ver al final del anexo \nameref{anexo:kafka}, arranqué el servicio y
creé el tópico que voy a usar para la ingesta de datos de SmartParking:
\fig[1\textwidth]{Kafka/Screenshot_8.png}

Con todo esto, ya estaría finalizada la configuración de Kafka, así que realicé una prueba básica en la que creo un \textbf{productor}
que envía mensajes de prueba y un \textbf{consumidor} que los va a consumir, y el resultado fue el siguiente:
\fig[1\textwidth]{Kafka/Screenshot_10.png}
\begin{center}
  {\small(Anexo: \nameref{anexo:kafka})}.
\end{center}

\subsection{MongoDB}
Siguiendo el orden natural del proyecto, lo siguiente sería \textbf{Apache NiFi}, pero sería complicado hacer el flujo sin tener listo el destino de los datos,
así que antes me encargué de configurar MongoDB.

Lo primero fue añadirlo a los repositorios por defecto de linux e instalarlo con apt.
\fig[1\textwidth]{MongoDB/Screenshot_3.png}
\begin{center}
  {\small(Anexo: \nameref{anexo:mongo})}.
\end{center}

Tras la instalación, habilité el servicio y accedí a Mongo con \texttt{mongosh}. Dentro creé la base de datos y ambas colecciones para el proyexto:
\begin{itemize}
  \item \textbf{events} para almacenar el histórico de las plazas del parking.
  \item \textbf{bays} para almacenar el estado de cada plaza en tiempo real.
\end{itemize}
\fig[1\textwidth]{MongoDB/Screenshot_5.png}

Con esto, MongoDB ya estaría listo para su uso, pero antes de continuar hice algunas pruebas de inserción de datos consultas y upsert. Finalmente añadí índices
para facilitar las consultas:
\fig[1\textwidth]{MongoDB/Screenshot_8.png}
\begin{center}
  {\small(Anexo: \nameref{anexo:mongo})}.
\end{center}

\newpage

\subsection{Apache NiFi}
Teniendo listos tanto el origen como el destino de los datos, configuré la conexión entre ambos, es decir, el flujo de \textbf{Apache NiFi}.

Antes de comenzar a hacer el flujo, instalé y configuré NiFi, de forma que este empleara Java 21, no la versión por defecto del sistema.
Además, hice algunos cambios en el archivo de propiedades para facilitar la conexión al servicio.
\fig[0.9\textwidth]{NiFi/Screenshot_6_7.png}
\begin{center}
  {\small(Anexo: \nameref{anexo:nifi})}.
\end{center}

\subsubsection{Flujo de Datos}
Llegamos a lo que sería el corazón del proyecto, la parte que conecta la ingesta de datos con el almacén de los mismos: el flujo de datos de NiFi.

El flujo comienza con un procesador \textbf{ConsumeKafka} que, como su nombre indica, consume de un tópico de Kafka todos los mensajes que llegan.

En la configuración de este procesador, hay que especificar las siguientes propiedades:
\fig[0.8\textwidth]{NiFi/Screenshot_8.png}
\begin{center}
  {\small(Anexo: \nameref{anexo:nifi})}.
\end{center}

Con esto, el flujo ya debería consumir los mensajes que se envían a través del tópico de Kafka. A partir de aquí, el flujo se divide en dos.
Por un lado, los mensajes irían directos a la colección \textbf{events}, ya que al ser un histórico almacena todos los eventos que ocurren en el parking.
Por otro lado, los mensajes irían a la colección \textbf{bays}, que se iría actualizando haciendo \textbf{upserts} de los mensajes para mantener el estado 
en tiempo real de cada plaza del parking.

Primero hice la parte relacionada a la colección \textbf{events}. Por cuestiones de rendimiento y ahorro de recursos, dado que esta colección no es necesario que esté
en tiempo real, decidí poner un procesador \textbf{MergeContent}. Este procesador va recibiendo los mensajes de Kafka a través del procesador y los va guardando
hasta que tiene entre 50 y 1000 mensajes. Entonces los fusiona en un solo JSON y lo envía al siguiente procesador. De esta forma, a nivel de MongoDB, en la colección \textbf{events}
se estarían haciendo pocos inserts de muchos mensajes, en lugar de un insert por cada mensaje.
\fig[0.8\textwidth]{NiFi/Screenshot_22.png}
\begin{center}
  {\small(Anexo: \nameref{anexo:nifi})}.
\end{center}

El siguiente procesador es un \textbf{PutMongoRecord} que almacena los mensajes que recibe en la colección \textbf{events}. 
\fig[0.8\textwidth]{NiFi/Screenshot_12.png}
\begin{center}
  {\small(Anexo: \nameref{anexo:nifi})}.
\end{center}

En este momento el flujo tenía esta forma:
\fig[0.8\textwidth]{NiFi/Screenshot_16.png}

Al llegar a este punto, hice una prueba para comprobar si hasta ahora todo funcionaba correctamente. Para este prueba utilicé un script en python --Anexo: \nameref{anexo:script-sendMessages}-- que simula el comportamiento
de los sensores IoT, enviando un mensaje con todos los datos aleatorizados a través del tópico de Kafka.

El resultado fue el esperado, al llegar a los 50 mensajes enviados, se insertaban en la colección events de MongoDB:
\fig[0.8\textwidth]{NiFi/Screenshot_17.png}

A continuación, seguí con la otra parte del flujo, para almacenar los mensajes en tiempo real de cada plaza del parking. Para ello usé otro procesador \textbf{PutMongoRecord}
para que fuese almacenando los mensajes en la colección \textbf{bays} a medida que iban llegando, con la diferencia de que antes de insertar, compruebe si el \textbf{bay\_id}
ya existe en la colección, para hacer un upsert.
\fig[1\textwidth]{NiFi/Screenshot_19.png}
\begin{center}
  {\small(Anexo: \nameref{anexo:nifi})}.
\end{center}

El flujo final acabaría siendo este:
\fig[1\textwidth]{NiFi/Screenshot_20.png}

\newpage

Finalmente, con la base de datos vacía de nuevo, hice de nuevo la misma prueba que antes, con la diferencia de que esta vez, la colección \textbf{bays} se iba llenando
con los mensajes que llegaban en tiempo real, mientras que la colección \textbf{events} se iba llenando como se ha visto anteriormente, de 50 en 50 mensajes. Por lo que,
al inicio la colección events está vacía mientras que la de bays empieza a recibir JSONs:
\fig[1\textwidth]{NiFi/Screenshot_23.png}

Con esto terminé la parte principal del proyecto, así que continué con la parte de mostrar los datos almacenados, primero las plazas en tiempo real y posteriormente la parte
de mostrar información a partir de los datos históricos.

\subsection{Parking en tiempo real con Flask}
Con el componente principal del sistema funcionando, me encargué de la parte relacionada con \textbf{representar los datos}, primero con Flask para mostrar las plazas de parking
en tiempo real y posteriormente con Dremio para obtener estádisticas y patrones de los datos históricos.

Este apartado se divide en dos partes:
\begin{enumerate}
  \item El \textbf{backend} en python que obtiene datos de Mongo y los envía al front.
  \item El \textbf{frontend} que recibe los datos del backend y representa un mapa del parking en tiempo real.
\end{enumerate}

El backend se conecta con la colección bays de la base de datos almacenada en MongoDB y extrae los datos. Posteriormente crea un 
endpoint para acceder a los datos, parseando algunos campos como el bay\_id para facilitar el trabajo del front. Finalmente crea el
endpoint para acceder a la web e inicia un servidor web en el puerto 5000.
{\small(Anexo: \nameref{anexo:flask-backend})}.

El frontend recoge los datos del backend y hace un mapa que se actualiza cada cierto tiempo, a elección del usuario entre 1, 3 o 5 segundos,
con la posibilidad de forzar una actualización. El mapa ordena las plazas por su bay\_id, dividiendolas por niveles y mostrandolas en orden 
según el número de la plaza. El color de la misma indica si está libre (verde) u ocupada (rojo), además de mostrar la fecha y hora de
última actualización de la misma. También muestra información sobre algunos datos relevantes sacados a partir de operaciones previas
hechas en el backend, como indicar el número de plazas totales y las que están libres y ocupadas:

El resultado final de la web fue el siguiente:
\fig[1\textwidth]{flask/Screenshot_1.png}

\newpage

\subsection{Análisis y estádistica de los datos con Dremio}
El último punto de este proyecto, como ya mencioné anteriormente, es configurar Dremio para consultar los datos históricos de las plazas del parking, de forma que
podamos \textbf{analizar y sacar conclusiones} de los mismos, como patrones que se repiten, estadísticas de interés, etc. 

\subsubsection{Instalación y configuración general}

\subsubsection{Configuración específica}

\subsubsection{Consultas de ejemplo}
\section{Fase de pruebas}
// TODO: Describir las pruebas realizadas y los resultados obtenidos aquí.

\section{Conclusiones y líneas futuras}
\subsection{Conclusiones}
Este proyecto ha demostrado la viabilidad de uso de tecnologías Big Data e IoT para la gestión inteligente de aparcamientos urbanos.
La integración de Apache Kafka, Apache NiFi, MongoDB, Flask y Dremio ha permitido desarrollar un sistema capaz de capturar, procesar
y visualizar datos en tiempo real, mejorando la eficiencia en la gestión del espacio de aparcamiento.

\subsection{Líneas futuras}
Las futuras mejoras y expansiones del proyecto SmartParking podrían incluir:
\begin{itemize}
  \item Integración con sistemas de pago y reservas de plazas.
  \item Desarrollo de una aplicación móvil nativa para usuarios.
  \item Implementación de algoritmos de predicción basados en aprendizaje automático.
  \item Expansión del sistema a múltiples ubicaciones y ciudades.
  \item Incorporación de análisis avanzados y generación de informes personalizados.
  \item Optimización del rendimiento y escalabilidad del sistema.
\end{itemize}

\subsection{Lecciones aprendidas}
El desarrollo del proyecto SmartParking ha proporcionado valiosas lecciones en la integración de tecnologías Big Data e IoT,
destacando la importancia de una planificación cuidadosa, pruebas exhaustivas y la adaptabilidad a los desafíos técnicos que surgen durante la implementación.

\begin{itemize}
  \item La orquestación con NiFi simplifica significativamente el manejo de flujo de datos complejos.
  \item MongoDB ofrece el equilibrio adecuado entre flexibilidad y rendimiento para datos de sensores IoT.
  \item La documentación y pruebas continuas son esenciales para garantizar la calidad del sistema.
\end{itemize}

\section{Bibliografía}
\begin{itemize}
  \item Documentación oficial de Apache NiFi: \url{https://nifi.apache.org/docs.html}
  \item Documentación oficial de Apache Kafka: \url{https://kafka.apache.org/documentation/}
  \item Documentación oficial de MongoDB: \url{https://docs.mongodb.com/}
  \item Documentación oficial de Flask: \url{https://flask.palletsprojects.com/en/2.0.x/}
  \item Documentación oficial de Dremio: \url{https://docs.dremio.com/}
  \item Herramientas de IA Generativa:
  \begin{itemize}
    \item ChatGPT de OpenAI: \url{https://openai.com/chatgpt}
    \item DeepSeek: \url{https://www.deepseek.com/}
    \item Copilot de GitHub (Integrado en VS Code).
  \end{itemize}
\end{itemize}

\subsection*{Nota sobre el uso de IA Generativa}
El uso de estas herramientas se ha limitado a la resolución de dudas puntuales, generación de código (Script de envío de mensajes y Flask) y
apoyo en la redacción de ciertos apartados del documento. Las respuestas obtenidas de estas herramientas se han contrastado con fuentes oficiales
y se han usado a modo orientativo.

\section{Diario de trabajo}
\begin{itemize}
  \item Semana 1: Análisis y diseño del sistema:
  \begin{itemize}
    \item Investigación de tecnologías requeridas.
    \item Definición de requisitos funcionales y no funcionales.
    \item Diseño de la arquitectura del sistema y la base de datos.
  \end{itemize}
  \item Semana 2: Configuración de Apache Kafka, Apache NiFi e implementación de la base de datos.
  \begin{itemize}
    \item Configuración de Kafka y creación de tópicos.
    \item Implementación de la base de datos MongoDB y creación de colecciones.
    \item Diseño e implementación del flujo de datos en NiFi.
    \item Pruebas iniciales de ingesta y almacenamiento de datos.
  \end{itemize}
  \item Semana 3: Desarrollo del backend y la interfaz web e integración con dremio.
  \begin{itemize}
    \item Investigación de Flask y desarrollo de la aplicación web.
    \item Integración de Dremio para análisis de datos y desarrollo de consultas descriptivas.
  \end{itemize}
  \item Semana 4: Pruebas finales, desarrollo de la memoria y video demostrativo.
  \begin{itemize}
    \item Realización de pruebas de carga y rendimiento.
    \item Documentación del proyecto y elaboración de la memoria.
    \item Creación del video demostrativo del sistema.
  \end{itemize}
\end{itemize}

\section{Anexo}

\subsection{Configuración del entorno}
\subsubsection*{Paquetes necesarios}
\label{anexo:paquetes-vm}

Instalación de paquetes necesarios:
\begin{lstlisting}
  $ sudo apt update && sudo apt upgrade -y
  $ sudo apt install -y curl wget git unzip build-essential jq python3 python3-venv python3-pip net-tools
\end{lstlisting}

\subsubsection*{Instalación y configuración de Java 11 \& 21}
\label{anexo:config-java}

Instalación de Java 11:
\begin{lstlisting}
  $ sudo apt install -y openjdk-11-jdk
\end{lstlisting}
\fig[1\textwidth]{Preparación de la VM/Screenshot_4.png}

Se incluye la variable \texttt{JAVA\_HOME} en el archivo \texttt{.bashrc}:
\fig[1\textwidth]{Preparación de la VM/Screenshot_5.png}

Instalación de Java 21:
\begin{lstlisting}
  $ sudo apt install -y openjdk-21-jdk
\end{lstlisting}

Especificar la versión 11 como predeterminada del sistema:
\begin{lstlisting}
  $ sudo update-alternatives --config java
\end{lstlisting}
\fig[1\textwidth]{Preparación de la VM/Screenshot_7.png}
\fig[1\textwidth]{Preparación de la VM/Screenshot_8.png}

\subsection{Apache Kafka}
\label{anexo:kafka}

Descarga y ubicación de Apache Kafka:
\fig[1\textwidth]{Kafka/Screenshot_1.png}

Adición de la variable de entorno \texttt{KAFKA\_HOME} al archivo \texttt{.bashrc}:
\fig[1\textwidth]{Kafka/Screenshot_2.png}

\subsubsection*{Explicación del archivo de configuración:}

\fig[1\textwidth]{Kafka/Screenshot_4.png}
Este archivo configura Kafka en modo autónomo (sin Zookeeper) para desarrollo local. Define un único nodo que actúa como broker y controller,
escuchando en el puerto 9092 para clientes y en el 9093 para comunicación interna, con los datos almacenados en una carpeta específica.

\subsubsection*{Configuración final de Kafka:}

Generar el \texttt{cluster.id}:
\fig[1\textwidth]{Kafka/Screenshot_5.png}

Formatear el almacenamiento:
\fig[1\textwidth]{Kafka/Screenshot_6.png}

Verificar el \texttt{meta.properties} para ver que está todo correcto y arranco el servicio:
\fig[1\textwidth]{Kafka/Screenshot_7.png}

\subsubsection*{Explicación de la prueba final de Kafka:}

Crear un \textbf{productor} que usado para enviar algunos mensajes de prueba, usando el topico creado para el proyecto:
\fig[1\textwidth]{Kafka/Screenshot_9.png}

Crear un \textbf{consumidor} que lee los mensajes desde el inicio del canal de datos, para comprobar si se han enviado correctamente y si 
llegan a través del canal:
\fig[1\textwidth]{Kafka/Screenshot_10.png}

\subsection{MongoDB}
\label{anexo:mongo}

\subsubsection*{Adición del repositorio de MongoDB}

\begin{lstlisting}
$ wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc | sudo apt-key add -
$ echo "deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/6.0 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-6.0.list
$ sudo apt update
\end{lstlisting}

\subsubsection*{Probando MongoDB}

Prueba de inserción:
\fig[1\textwidth]{MongoDB/Screenshot_6.png}

Prueba de consulta:
\fig[1\textwidth]{MongoDB/Screenshot_7.png}

Prueba de upsert:
\fig[1\textwidth]{MongoDB/Screenshot_9.png}

\subsection{Apache NiFi}
\label{anexo:nifi}

\subsubsection*{Instalación y configuración inicial de NiFi}

Descarga y descompresión de NiFi:
\fig[1\textwidth]{NiFi/Screenshot_1.png}

Adición de la variable de entorno \texttt{NIFI\_HOME} al archivo \texttt{.bashrc}:
\fig[1\textwidth]{NiFi/Screenshot_2.png}

En el entorno de NiFi, especificar que use Java 21:
\fig[1\textwidth]{NiFi/Screenshot_3.png}

Configuración de las propiedades para cambiar el uso de \texttt{https} por \texttt{http} 
y facilitar el acceso al servicio:
\fig[1\textwidth]{NiFi/Screenshot_4.png}
\fig[1\textwidth]{NiFi/Screenshot_5.png}
\fig[1\textwidth]{NiFi/Screenshot_6.png}

\subsubsection*{Explicación detallada de la configuración del flujo}

El flujo comienza con un procesador ConsumeKafka:
\fig[1\textwidth]{NiFi/Screenshot_7.png}

Este flujo, recibe el servicio de conexión con Kafka y el tópico en el cual tiene leer, entre otros datos no tan relevantes:
\fig[1\textwidth]{NiFi/Screenshot_8.png}

Sobre el servicio de conexión con Kafka, es necesario crearlo y configurarlo. En su configuración se especifican los servidores de
bootstrap:
\fig[1\textwidth]{NiFi/Screenshot_9.png}

Una vez hecho, es necesario habilitarlo para que funcione.

El procesador ConsumeKafka va directamente conectado en la relación \textbf{success} a un procesador MergeRecord, que fusionara los mensajes recibidos
en formato a JSON a un solo JSON, de 50 en 50.
\fig[1\textwidth]{NiFi/Screenshot_24.png}

La configuración de este procesador recibe dos servicios: uno de lectura y otro de escritura, ambos de JSON, ya que es el tipo de documento con el que
estoy trabajando, además de especificar el límite inferior y superior de mensajes a fusionar.
\fig[1\textwidth]{NiFi/Screenshot_22.png}

Estos servicios también hay que crearlos, configurarlos y habilitarlos. Para ambos, su configuración se limita a especificar el formato de fecha, hora y
zona horaria:
\fig[1\textwidth]{NiFi/Screenshot_14.png}

Este procesador anterior, va conectado a un procesador PutMongoRecord que se encargará de guardar en la colección que se le indique los mensajes que reciba:
\fig[1\textwidth]{NiFi/Screenshot_11.png}

En su configuración hay que especificar el servicio de conexión con MongoDB, la base de datos, la colección y el servicio de lectura de JSON (sirve el mismo
que se ha creado anteriormente):
\fig[1\textwidth]{NiFi/Screenshot_12.png}

En el servicio de conexión con MongoDB solo hay que especificar la url de conexión con el mismo, para posteriormente habilitarlo:
\fig[1\textwidth]{NiFi/Screenshot_13.png}

Ahora, para la otra parte del flujo, he usado otro procesador PutMongoRecord, con la misma configuración que el anterior, pero cambiando la colección por
la de bays. Además, como campo clave para actualizar he indicado el \textbf{bay\_id}, para que actualice o inserte el mensaje dependiendo de si ya existe
o no un JSON con ese id. El modo de actualización sirve para indicar si solo actualizar uno o muchos en caso de que haya varios con el mismo valor del campo.
En este caso es indiferente ya que en teoría no se debería repetir y siempre se va a actualizar uno solo, pero por si acaso lo dejé en uno.
\fig[1\textwidth]{NiFi/Screenshot_19.png}

El resultado final del flujo de NiFi funcional con todos los requerimientos es el siguiente:
\fig[1\textwidth]{NiFi/Screenshot_20.png}

\subsubsection*{Script de envío de mensajes}
\label{anexo:script-sendMessages}
\fig[1\textwidth]{python/sendMessages.png}
{\tiny*Código generado por IA generativa}

\subsection{Back-end de Flask}
\label{anexo:flask-backend}
\fig[0.9\textwidth]{python/app.png}
{\tiny*Código generado por IA generativa}

\subsection{Front-end de Flask}
\label{anexo:flask-frontend}
\subsubsection*{CSS}
\fig[0.5\textwidth]{python/css.png}
{\tiny*Código generado por IA generativa}

sdasdasdas

\subsubsection*{HTML}
\fig[0.6\textwidth]{python/html.png}
{\tiny*Código generado por IA generativa}

\subsubsection*{JavaScript}
\fig[0.55\textwidth]{python/javascript.png}
{\tiny*Código generado por IA generativa}

\end{document}